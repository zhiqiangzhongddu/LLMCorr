{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78684b2d3cd110ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T21:26:02.946895Z",
     "start_time": "2024-01-25T21:25:57.487014Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "import os\n",
    "\n",
    "from code.config import cfg, update_cfg\n",
    "from code.utils import time_logger\n",
    "from code.query_chatgpt import num_tokens_from_messages, get_context_window_size_limit\n",
    "from code.data_utils.utils import (load_message, check_cache_response,\n",
    "                                   save_chatcompletion, save_response,\n",
    "                                   load_chatcompletion, load_response,\n",
    "                                   clean_cache_chat_completion_response)\n",
    "from code.data_utils.dataset import DatasetLoader\n",
    "from code.query_chatgpt_explainer import query_chatgpt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cee70b4193656e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T21:26:02.951935Z",
     "start_time": "2024-01-25T21:26:00.234853Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# manual cfg settings\n",
    "cfg.dataset = \"ogbg-molbace\" # ogbg-molhiv\n",
    "cfg.llm.template = \"EP\"\n",
    "# cfg.llm.model.name = \"gpt-4-1106-preview\" # gpt-3.5-turbo-1106, gpt-4-1106-preview \n",
    "cfg.demo_test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1232ccff7fec5f24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T21:26:02.956116Z",
     "start_time": "2024-01-25T21:26:00.237866Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RPM limit (adjust according to your plan)\n",
    "rpm_limit = 3500\n",
    "# TPM limit (adjust according to your plan)\n",
    "tpm_limit = 60000\n",
    "# Context window size\n",
    "cws_limit = get_context_window_size_limit(cfg.llm.model.name)\n",
    "\n",
    "# Set up OpenAI API\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    api_version=\"2023-12-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    ") if cfg.llm.provider == 'aoai' else OpenAI(api_key=cfg.OPENAI_API_KEY)\n",
    "\n",
    "message_list = load_message(\n",
    "    dataset_name=cfg.dataset, message_type=cfg.llm.template,\n",
    "    gnn_model=cfg.gnn.model.name, seed=cfg.seed,\n",
    "    demo_test=cfg.demo_test\n",
    ")\n",
    "if cfg.demo_test:\n",
    "    message_list = message_list[:cfg.num_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dfa49b2376df07f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T21:26:02.956775Z",
     "start_time": "2024-01-25T21:26:00.286280Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an AI molecule property analysis assistant specializing in explaining predictions made by Graph Neural Networks (GNNs). The prediction is about whether the molecule inhibits human β-secretase 1(BACE-1).'},\n",
       " {'role': 'user',\n",
       "  'content': \"The molecule-241's SMILES string is s1cc(cc1)-c1cc2c(nc(N)cc2)cc1. The molecule-241's prediction given by GNNs is False with predicted probability 0.9668. The true label of the molecule-241 is True. Indentify insightful features to explain why GNNs make this prediction for molecule-241.\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ecd71920061059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T21:26:02.960539Z",
     "start_time": "2024-01-25T21:26:00.286552Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7039d1726abe7dcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T21:28:22.818161Z",
     "start_time": "2024-01-25T21:26:00.286584Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query ogbg-molbace gpt-3.5-turbo-1106:   5%|▌         | 1/20 [00:00<00:07,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory /home/zhiqiang/LLMaGML/output/chat_completion/ogbg-molbace/cache_chat_completion/EP-gin-v-42/\n",
      "Created directory /home/zhiqiang/LLMaGML/output/response/ogbg-molbace/cache_response/EP-gin-v-42/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query ogbg-molbace gpt-3.5-turbo-1106: 100%|██████████| 20/20 [02:22<00:00,  7.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# Save all queries\n",
    "chat_completion_list = []\n",
    "response_list = []\n",
    "\n",
    "# Run batch queries\n",
    "batch_message_list = []\n",
    "batch_message_token_num = 0\n",
    "batch_start_id = 0\n",
    "display = \"Query {} {}\".format(cfg.dataset, cfg.llm.model.name)\n",
    "for message_id, message in enumerate(tqdm(message_list, desc=display)):\n",
    "\n",
    "    num_tokens = num_tokens_from_messages(\n",
    "        messages=message, original_model=cfg.llm.model.name\n",
    "    )\n",
    "    if num_tokens > tpm_limit:\n",
    "        sys.exit(\"Message token number is large than limit {}.\".format(tpm_limit))\n",
    "    while num_tokens >= cws_limit:\n",
    "        print(\"Message context length is {}, larger than Context Window Size limit {}.\".format(\n",
    "            num_tokens, cws_limit\n",
    "        ))\n",
    "        str_len = len(message[1][\"content\"])\n",
    "        message[1][\"content\"] = (message[1][\"content\"][:int(str_len / 3)] +\n",
    "                                 message[1][\"content\"][-int(str_len / 3):])\n",
    "        num_tokens = num_tokens_from_messages(\n",
    "            messages=message, original_model=cfg.llm.model.name\n",
    "        )\n",
    "        print(\"Message token number is reduced to {}.\".format(num_tokens))\n",
    "    batch_message_token_num += num_tokens\n",
    "\n",
    "    if (batch_message_token_num >= tpm_limit) and (message_id < len(message_list) - 1):\n",
    "\n",
    "        batch_chat_completion_list, batch_response_list = query_chatgpt_batch(\n",
    "            client=client, dataset_name=cfg.dataset,\n",
    "            llm_model=cfg.llm.model.name, template=cfg.llm.template,\n",
    "            gnn_model=cfg.gnn.model.name, seed=cfg.seed,\n",
    "            batch_message_list=batch_message_list, batch_start_id=batch_start_id,\n",
    "            rpm_limit=rpm_limit\n",
    "        )\n",
    "        chat_completion_list += batch_chat_completion_list\n",
    "        response_list += batch_response_list\n",
    "        batch_message_list = [message]\n",
    "        batch_message_token_num = num_tokens_from_messages(\n",
    "            messages=message, original_model=cfg.llm.model.name\n",
    "        )\n",
    "        batch_start_id = message_id\n",
    "\n",
    "    elif message_id == len(message_list) - 1:\n",
    "        batch_message_list.append(message)\n",
    "\n",
    "        batch_chat_completion_list, batch_response_list = query_chatgpt_batch(\n",
    "            client=client, dataset_name=cfg.dataset,\n",
    "            llm_model=cfg.llm.model.name, template=cfg.llm.template,\n",
    "            gnn_model=cfg.gnn.model.name, seed=cfg.seed,\n",
    "            batch_message_list=batch_message_list, batch_start_id=batch_start_id,\n",
    "            rpm_limit=rpm_limit\n",
    "        )\n",
    "        chat_completion_list += batch_chat_completion_list\n",
    "        response_list += batch_response_list\n",
    "\n",
    "    else:\n",
    "        batch_message_list.append(message)\n",
    "        \n",
    "# Save all chat completion\n",
    "save_chatcompletion(\n",
    "    dataset_name=cfg.dataset, chat_completion=chat_completion_list,\n",
    "    gnn_model=cfg.gnn.model.name, seed=cfg.seed,\n",
    "    template=cfg.llm.template, demo_test=cfg.demo_test\n",
    ")\n",
    "# Save all responses\n",
    "save_response(\n",
    "    dataset_name=cfg.dataset, list_response=response_list,\n",
    "    gnn_model=cfg.gnn.model.name, seed=cfg.seed,\n",
    "    template=cfg.llm.template, demo_test=cfg.demo_test\n",
    ")\n",
    "# Clean all cache files\n",
    "clean_cache_chat_completion_response(\n",
    "    dataset_name=cfg.dataset, template=cfg.llm.template,\n",
    "    gnn_model=cfg.gnn.model.name, seed=cfg.seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67127413eaacdb58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-25T21:28:22.917575Z",
     "start_time": "2024-01-25T21:28:22.916637Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
